# --- Defaults ---

# --- pymarl options, 这些参数有可能不环境和算法的重复配置覆盖掉 ---
runner: "episode" # 运行1个env，如果是parallel是并发运行多个env
mac: "basic_mac" # 基本控制器
env: "sc2" # env的名字，sc2是星际争霸2, 'stag_hunt'是捕猎者游戏
env_args: {} # env的一些参数
batch_size_run: 1 # 并发运行的环境数量
test_nepisode: 20 # 测试的episode量
test_interval: 2000 # 多少个timeSteps之后开始测试
test_greedy: True # 使用贪婪评估（如果是False，将epsilon设置为0)
log_interval: 2000 # 每个{}时间步后记录统计数据日志
runner_log_interval: 2000 #每个{}时间步来记录runner stats（不是测试统计数据）
learner_log_interval: 2000 # 每个{} timesteps的记录训练统计信息
t_max: 10000 # 在这个时间下停止运行
use_cuda: True # 默认情况下使用GPU，除非它不可用
buffer_cpu_only: True # 如果是True，我们就不会把所有的回放缓冲区都保留在vram中。

# --- 日志选项 ---
use_tensorboard: False # 日志结果到Tensorboard
save_model: False # 是否将模型保存到磁盘
save_model_interval: 2000000 # 多少个时间步后保存模型
checkpoint_path: "" # 从此路径加载checkpoint
evaluate: False # 评估测试test_nepisode 个episode，然后退出
load_step: 0 # 加载在这个时间段内训练的模型（如果选择最大可能，则为0)
save_replay: False # 保存从checkpoint_path加载的模型的重放
local_results_path: "results" # 本地存储results的路径

# --- 强化学习超参数 ---
gamma: 0.99
batch_size: 32 # 训练的episode数
buffer_size: 32 # 重放缓冲区的大小
lr: 0.0005 # agent学习率
critic_lr: 0.0005 # critic的学习率
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 10 # 减少该L2 norm 以上的梯度幅度

# --- Agent parameters ---
agent: "rnn" # 默认rnn的agent
rnn_hidden_dim: 64 # 默认RNN agent的隐藏状态大小
obs_agent_id: True # 在观察中包含agent的one_hot ID
obs_last_action: True # 在观察中包括agent的最后一个动作（one_hot）

# --- 实验运行参数 ---
repeat_id: 1
label: "default_label"

run: "default"
